{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import pandas\n",
    "import re\n",
    "import string\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data and HurtLex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pandas.read_csv('data.csv')\n",
    "lexicon = pandas.read_csv('New_Lexicon.csv')\n",
    "hurt_words = set(lexicon['clean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Models\n",
    "Start with converting text into feature vectors using Count Vectorizer and TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode the labels using preprocessing.LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = preprocessing.LabelEncoder()\n",
    "targets = encoder.fit_transform(df['sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then split the data for training and testing. Use 70% for training and 30% for testing. Since the collected data comes from multiple resources, shuffle it so the order won't affect the classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into training and test datasets 70% training, 30% testing\n",
    "train_x, test_x, train_y, test_y = model_selection.train_test_split(df['clean'].values.astype('U'), targets,test_size=0.3, random_state = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To extract Count Vectors, simply use CountVectorizer function from sklearn.feature_extraction.tex. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that, fitting the Vectorizer is on training_set only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a count vectorizer object \n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "count_vect.fit(train_x)\n",
    "\n",
    "# transform the training and test data using count vectorizer object\n",
    "xtrain_count =  count_vect.transform(train_x)\n",
    "xtest_count =  count_vect.transform(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same idea for TF-IDF Vectorizer. Use TfidfVectorizer for Word-Level, N-gram-Level, Char-Level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word level tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "tfidf_vect.fit(train_x)\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "xtest_tfidf =  tfidf_vect.transform(test_x)\n",
    "\n",
    "# ngram level tf-idf \n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3))\n",
    "tfidf_vect_ngram.fit(train_x)\n",
    "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\n",
    "xtest_tfidf_ngram =  tfidf_vect_ngram.transform(test_x)\n",
    "\n",
    "# characters level tf-idf\n",
    "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', ngram_range=(2,3))\n",
    "tfidf_vect_ngram_chars.fit(train_x)\n",
    "xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_x) \n",
    "xtest_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(test_x) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with NLP based feature model.\n",
    "\n",
    "Since Arabic has its own characters, extend the existing punctuation list, which only contains the English punctuations, and add the Arabic punctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "arabic_punctuations = '''`÷×؛<>_()*&^%][ـ،/:\"؟.,'{}~¦+|!”…“–ـ'''\n",
    "english_punctuations = string.punctuation\n",
    "punctuations_list = arabic_punctuations + english_punctuations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The extracted features from the text are:\n",
    "\n",
    "- char_count : number of characters in the tweet.\n",
    "- word_count : number of words in the tweet.\n",
    "- word_density : average word length in the tweet.\n",
    "- punctuation_count : number of punctuations in the tweet.\n",
    "- hashtag : number of hash tags in the tweet.\n",
    "- hate_words_count : count of HurtLex words in the tweet.\n",
    "- hate_word : a binary flag to indicate whether a tweet contains a word from HurtLex or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['char_count'] = df['tweet'].apply(len)\n",
    "df['word_count'] = df['tweet'].apply(lambda x: len(x.split()))\n",
    "df['word_density'] = df['char_count'] / (df['word_count'])\n",
    "df['punctuation_count'] = df['tweet'].apply(lambda x: len(\"\".join(_ for _ in x if _ in punctuations_list)))\n",
    "df['hashtag'] = df['tweet'].apply(lambda x: len(re.findall(r\"#(\\w+)\", x)))\n",
    "df['hate_words_count'] = df['clean'].apply(lambda x: len([wrd for wrd in hurt_words if wrd in str(x)]))\n",
    "df['hate_word'] = df['clean'].apply(lambda x: 1 if (len([wrd for wrd in hurt_words if wrd in str(x)])) > 0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "NLP_features = ['char_count', 'word_count', 'word_density', 'punctuation_count', 'hate_words_count', 'hate_word', 'hashtag']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use 70% of the extracted features for training and 30% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into training and test datasets 70% training, 30% testing\n",
    "NLP_train_x, NLP_test_x, NLP_train_y, NLP_test_y = model_selection.train_test_split(df[NLP_features], targets, test_size=0.3, random_state = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a train_test_model function. Pass the model, train_set, train_target, test_set and test_target as parameters. The function returns four metric scores: Accuracy, Precision, Recall, F1_Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_model(model, train_set, train_target, test_set, test_target, final=False, model_name=None):\n",
    "    \n",
    "    # fit the training dataset on the classifier\n",
    "    model.fit(train_set, train_target)\n",
    "    if final:\n",
    "        pickle.dump(model, open(model_name, 'wb'))\n",
    "    # GET PREDICTED VALUES\n",
    "    test_predictions = model.predict(test_set)\n",
    "    \n",
    "    # GET EVALUATION NUMBERS ON TRAIN SET \n",
    "    accuracy = metrics.accuracy_score(test_target, test_predictions, normalize=True)\n",
    "    precision = metrics.precision_score(test_target, test_predictions, average='macro', zero_division='warn')\n",
    "    recall = metrics.recall_score(test_target, test_predictions, average='macro', zero_division='warn')\n",
    "    f1 = metrics.f1_score(test_target, test_predictions, average='macro', zero_division='warn')\n",
    "    \n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB, Count Vectors: accuracy : 0.7434, precision : 0.6853, recall : 0.5156, f1_score : 0.5526\n",
      "NB, WordLevel TF-IDF: accuracy : 0.7105, precision : 0.8348, recall : 0.3981, f1_score : 0.3879\n",
      "NB, N-Gram Vectors: accuracy : 0.7019, precision : 0.8498, recall : 0.3850, f1_score : 0.3660\n",
      "NB, CharLevel Vectors: accuracy : 0.7032, precision : 0.7704, recall : 0.3916, f1_score : 0.3784\n",
      "NB, NLP Features: accuracy : 0.6682, precision : 0.4451, recall : 0.3803, f1_score : 0.3688\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes on Count Vectors\n",
    "accuracy, precision, recall, f1 = train_test_model(naive_bayes.MultinomialNB(), xtrain_count, train_y, xtest_count, test_y)\n",
    "print(\"NB, Count Vectors: accuracy : %.4f, precision : %.4f, recall : %.4f, f1_score : %.4f\" %(accuracy, precision,recall,f1))\n",
    "\n",
    "# Naive Bayes on Word Level TF IDF Vectors\n",
    "accuracy, precision, recall, f1 = train_test_model(naive_bayes.MultinomialNB(), xtrain_tfidf, train_y, xtest_tfidf, test_y)\n",
    "print (\"NB, WordLevel TF-IDF: accuracy : %.4f, precision : %.4f, recall : %.4f, f1_score : %.4f\" %(accuracy, precision,recall,f1))\n",
    "\n",
    "# Naive Bayes on Ngram Level TF IDF Vectors\n",
    "accuracy, precision, recall, f1 = train_test_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram, train_y, xtest_tfidf_ngram, test_y)\n",
    "print (\"NB, N-Gram Vectors: accuracy : %.4f, precision : %.4f, recall : %.4f, f1_score : %.4f\" %(accuracy, precision,recall,f1))\n",
    "\n",
    "# Naive Bayes on Character Level TF IDF Vectors\n",
    "accuracy, precision, recall, f1 = train_test_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram_chars, train_y, xtest_tfidf_ngram_chars, test_y)\n",
    "print (\"NB, CharLevel Vectors: accuracy : %.4f, precision : %.4f, recall : %.4f, f1_score : %.4f\" %(accuracy, precision,recall,f1))\n",
    "\n",
    "# Naive Bayes on NLP Features\n",
    "accuracy, precision, recall, f1 = train_test_model(naive_bayes.MultinomialNB(), NLP_train_x, NLP_train_y, NLP_test_x, NLP_test_y)\n",
    "print(\"NB, NLP Features: accuracy : %.4f, precision : %.4f, recall : %.4f, f1_score : %.4f\" %(accuracy, precision,recall,f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]LR, Count Vectors: accuracy : 0.7353, precision : 0.6474, recall : 0.5776, f1_score : 0.6023\n",
      "[LibLinear]LR, WordLevel TF-IDF: accuracy : 0.7515, precision : 0.6854, recall : 0.5534, f1_score : 0.5913\n",
      "[LibLinear]LR, N-Gram Vectors: accuracy : 0.7047, precision : 0.6530, recall : 0.4251, f1_score : 0.4357\n",
      "[LibLinear]LR, CharLevel Vectors: accuracy : 0.7468, precision : 0.6669, recall : 0.5629, f1_score : 0.5956\n",
      "[LibLinear]LR, NLP Features: accuracy : 0.6811, precision : 0.5743, recall : 0.3740, f1_score : 0.3525\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression on Count Vectors\n",
    "accuracy, precision, recall, f1 = train_test_model(linear_model.LogisticRegression(verbose=1, solver='liblinear',random_state=0, C=5, penalty='l2',max_iter=1000), xtrain_count, train_y, xtest_count, test_y)\n",
    "print (\"LR, Count Vectors: accuracy : %.4f, precision : %.4f, recall : %.4f, f1_score : %.4f\" %(accuracy, precision,recall,f1))\n",
    "\n",
    "# Logistic Regression on Word Level TF IDF Vectors\n",
    "accuracy, precision, recall, f1 = train_test_model(linear_model.LogisticRegression(verbose=1, solver='liblinear',random_state=0, C=5, penalty='l2',max_iter=1000), xtrain_tfidf, train_y, xtest_tfidf, test_y)\n",
    "print (\"LR, WordLevel TF-IDF: accuracy : %.4f, precision : %.4f, recall : %.4f, f1_score : %.4f\" %(accuracy, precision,recall,f1))\n",
    "\n",
    "# Logistic Regression on Ngram Level TF IDF Vectors\n",
    "accuracy, precision, recall, f1 = train_test_model(linear_model.LogisticRegression(verbose=1, solver='liblinear',random_state=0, C=5, penalty='l2',max_iter=1000), xtrain_tfidf_ngram, train_y, xtest_tfidf_ngram, test_y)\n",
    "print (\"LR, N-Gram Vectors: accuracy : %.4f, precision : %.4f, recall : %.4f, f1_score : %.4f\" %(accuracy, precision,recall,f1))\n",
    "\n",
    "# Logistic Regression on Character Level TF IDF Vectors\n",
    "accuracy, precision, recall, f1 = train_test_model(linear_model.LogisticRegression(verbose=1, solver='liblinear',random_state=0, C=5, penalty='l2',max_iter=1000), xtrain_tfidf_ngram_chars, train_y, xtest_tfidf_ngram_chars, test_y)\n",
    "print (\"LR, CharLevel Vectors: accuracy : %.4f, precision : %.4f, recall : %.4f, f1_score : %.4f\" %(accuracy, precision,recall,f1))\n",
    "\n",
    "# Linear Classifier on NLP Features\n",
    "accuracy, precision, recall, f1 = train_test_model(linear_model.LogisticRegression(verbose=1, solver='liblinear',random_state=0, C=5, penalty='l2',max_iter=1000), NLP_train_x, NLP_train_y, NLP_test_x, NLP_test_y)\n",
    "print (\"LR, NLP Features: accuracy : %.4f, precision : %.4f, recall : %.4f, f1_score : %.4f\" %(accuracy, precision,recall,f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM, Count Vectors: accuracy : 0.7124, precision : 0.6146, recall : 0.5773, f1_score : 0.5911\n",
      "SVM, WordLevel TF-IDF: accuracy : 0.7445, precision : 0.6655, recall : 0.5701, f1_score : 0.6024\n",
      "SVM, N-Gram Vectors: accuracy : 0.7055, precision : 0.6456, recall : 0.4419, f1_score : 0.4612\n",
      "SVM, CharLevel Vectors: accuracy : 0.7408, precision : 0.6577, recall : 0.5670, f1_score : 0.5973\n",
      "SVM, NLP Features: accuracy : 0.6754, precision : 0.3972, recall : 0.3587, f1_score : 0.3260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\salee\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# SVM on Count Vectors\n",
    "accuracy,precision,recall,f1 = train_test_model(svm.LinearSVC(), xtrain_count, train_y, xtest_count, test_y)\n",
    "print (\"SVM, Count Vectors: accuracy : %.4f, precision : %.4f, recall : %.4f, f1_score : %.4f\" %(accuracy, precision,recall,f1))\n",
    "\n",
    "# SVM on Word Level TF IDF Vectors\n",
    "accuracy,precision,recall,f1 = train_test_model(svm.LinearSVC(), xtrain_tfidf, train_y, xtest_tfidf, test_y)\n",
    "print (\"SVM, WordLevel TF-IDF: accuracy : %.4f, precision : %.4f, recall : %.4f, f1_score : %.4f\" %(accuracy, precision,recall,f1))\n",
    "\n",
    "# SVM on Ngram Level TF IDF Vectors\n",
    "accuracy,precision,recall,f1 = train_test_model(svm.LinearSVC(), xtrain_tfidf_ngram, train_y, xtest_tfidf_ngram, test_y)\n",
    "print (\"SVM, N-Gram Vectors: accuracy : %.4f, precision : %.4f, recall : %.4f, f1_score : %.4f\" %(accuracy, precision,recall,f1))\n",
    "\n",
    "# SVM on Character Level TF IDF Vectors\n",
    "accuracy, precision, recall, f1 = train_test_model(svm.LinearSVC(), xtrain_tfidf_ngram_chars, train_y, xtest_tfidf_ngram_chars, test_y)\n",
    "print (\"SVM, CharLevel Vectors: accuracy : %.4f, precision : %.4f, recall : %.4f, f1_score : %.4f\" %(accuracy, precision,recall,f1))\n",
    "\n",
    "# SVM on NLP Features\n",
    "accuracy,precision,recall,f1 = train_test_model(svm.LinearSVC(dual=False), NLP_train_x, NLP_train_y, NLP_test_x, NLP_test_y)\n",
    "print (\"SVM, NLP Features: accuracy : %.4f, precision : %.4f, recall : %.4f, f1_score : %.4f\" %(accuracy, precision,recall,f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine All Feature Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To combine our feature vectors, use the predictions of the previous vector models as features and add them to the NLP based features and re-train three new models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the predictions, fit the model on the whole dataset and predict on the same set since there is no need for training and testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_feature(model, train_set, train_target, name):\n",
    "    \n",
    "    # fit the training dataset on the classifier\n",
    "    model.fit(train_set, train_target)\n",
    "\n",
    "    # GET PREDICTED VALUES\n",
    "    train_predictions = model.predict(train_set)\n",
    "    with open(name, 'wb') as handle:\n",
    "        pickle.dump(model, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    return train_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_sth(sth, name):\n",
    "    with open(name, 'wb') as handle:\n",
    "        pickle.dump(sth, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the Count Vectors and TF-IDF Vectors again but this time, fit the vectorizers on the whole data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a count vectorizer object \n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "count_vect.fit(df['clean'].values.astype('U'))\n",
    "\n",
    "# transform the training and test data using count vectorizer object\n",
    "CV_features =  count_vect.transform(df['clean'].values.astype('U'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\salee\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:506: UserWarning: The parameter 'token_pattern' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    }
   ],
   "source": [
    "# word level tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "tfidf_vect.fit(df['clean'].values.astype('U'))\n",
    "tfidf_word_features =  tfidf_vect.transform(df['clean'].values.astype('U'))\n",
    "\n",
    "# ngram level tf-idf \n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3))\n",
    "tfidf_vect_ngram.fit(df['clean'].values.astype('U'))\n",
    "tfidf_ngram_features =  tfidf_vect_ngram.transform(df['clean'].values.astype('U'))\n",
    "\n",
    "# characters level tf-idf\n",
    "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3))\n",
    "tfidf_vect_ngram_chars.fit(df['clean'].values.astype('U'))\n",
    "tfidf_char_features =  tfidf_vect_ngram_chars.transform(df['clean'].values.astype('U')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_sth(count_vect, name='count_vect')\n",
    "# save_sth(tfidf_vect, name='tfidf_vect')\n",
    "# save_sth(tfidf_vect_ngram, name='tfidf_vect_ngram')\n",
    "# save_sth(tfidf_vect_ngram_chars, name='tfidf_vect_ngram_chars')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['NB_CountVector_feature'] = model_feature(naive_bayes.MultinomialNB(),CV_features, targets, name='NB_CountVector_feature')\n",
    "df['NB_tfidf_word_feature'] = model_feature(naive_bayes.MultinomialNB(),tfidf_word_features, targets, name='NB_tfidf_word_feature')\n",
    "df['NB_tfidf_ngram_feature'] = model_feature(naive_bayes.MultinomialNB(),tfidf_ngram_features, targets, name='NB_tfidf_ngram_feature')\n",
    "df['NB_tfidf_char_feature'] = model_feature(naive_bayes.MultinomialNB(),tfidf_char_features, targets, name='NB_tfidf_char_feature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['LR_CountVector_feature'] = model_feature(linear_model.LogisticRegression(solver='liblinear',random_state=0, C=5, penalty='l2',max_iter=1000),CV_features, targets, name='LR_CountVector_feature')\n",
    "df['LR_tfidf_word_feature'] = model_feature(linear_model.LogisticRegression(solver='liblinear',random_state=0, C=5, penalty='l2',max_iter=1000),tfidf_word_features, targets, name='LR_tfidf_word_feature')\n",
    "df['LR_tfidf_ngram_feature'] = model_feature(linear_model.LogisticRegression(solver='liblinear',random_state=0, C=5, penalty='l2',max_iter=1000),tfidf_ngram_features, targets, name='LR_tfidf_ngram_feature')\n",
    "df['LR_tfidf_char_feature'] = model_feature(linear_model.LogisticRegression(solver='liblinear',random_state=0, C=5, penalty='l2',max_iter=1000),tfidf_char_features, targets, name='LR_tfidf_char_feature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SVM_CountVector_feature'] = model_feature(svm.LinearSVC(), CV_features, targets, name='SVM_CountVector_feature')\n",
    "df['SVM_tfidf_word_feature'] = model_feature(svm.LinearSVC(), tfidf_word_features, targets, name='SVM_tfidf_word_feature')\n",
    "df['SVM_tfidf_ngram_feature'] = model_feature(svm.LinearSVC(), tfidf_ngram_features, targets, name='SVM_tfidf_ngram_feature')\n",
    "df['SVM_tfidf_char_feature'] = model_feature(svm.LinearSVC(), tfidf_char_features, targets, name='SVM_tfidf_char_feature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_features = ['char_count', 'word_count', 'word_density', 'punctuation_count', 'hate_words_count', 'hate_word', 'hashtag', \n",
    "            'NB_CountVector_feature', 'NB_tfidf_word_feature', 'NB_tfidf_ngram_feature','NB_tfidf_char_feature',\n",
    "            'LR_CountVector_feature','LR_tfidf_word_feature','LR_tfidf_ngram_feature','LR_tfidf_char_feature',\n",
    "            'SVM_CountVector_feature','SVM_tfidf_word_feature','SVM_tfidf_ngram_feature','SVM_tfidf_char_feature']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into training and test datasets 75% training, 25% testing\n",
    "train_x, test_x, train_y, test_y = model_selection.train_test_split(df[combined_features], targets, test_size=0.3, random_state = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save final feature dataset\n",
    "train_x.to_csv('train_x.csv')\n",
    "test_x.to_csv('test_x.csv')\n",
    "pandas.DataFrame({'sentiment': train_y}).to_csv('train_y.csv')\n",
    "pandas.DataFrame({'sentiment': test_y}).to_csv('test_y.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB, Combined Features: accuracy : 0.9184, precision : 0.8073, recall : 0.8506, f1_score : 0.8252\n",
      "[LibLinear]LR, Combined Features: accuracy : 0.9872, precision : 0.9780, recall : 0.9671, f1_score : 0.9725\n",
      "SVM, Combined Features: accuracy : 0.9851, precision : 0.9774, recall : 0.9635, f1_score : 0.9703\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes on Combined Features\n",
    "accuracy, precision, recall, f1 = train_test_model(naive_bayes.MultinomialNB(), train_x, train_y, test_x, test_y, final=True, model_name=\"NB\")\n",
    "print(\"NB, Combined Features: accuracy : %.4f, precision : %.4f, recall : %.4f, f1_score : %.4f\" %(accuracy, precision,recall,f1))\n",
    "\n",
    "# Linear Classifier on Combined Features\n",
    "accuracy, precision, recall, f1 = train_test_model(linear_model.LogisticRegression(verbose=1, solver='liblinear',random_state=0, C=5, penalty='l2',max_iter=1000), train_x, train_y, test_x, test_y, final=True, model_name=\"LR\")\n",
    "print (\"LR, Combined Features: accuracy : %.4f, precision : %.4f, recall : %.4f, f1_score : %.4f\" %(accuracy, precision,recall,f1))\n",
    "\n",
    "# SVM on Combined Features\n",
    "accuracy,precision,recall,f1 = train_test_model(svm.LinearSVC(dual=False), train_x, train_y, test_x, test_y, final=True, model_name=\"SVM\")\n",
    "print (\"SVM, Combined Features: accuracy : %.4f, precision : %.4f, recall : %.4f, f1_score : %.4f\" %(accuracy, precision,recall,f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = pickle.load(open(\"LR\", 'rb'))\n",
    "test_x = pandas.read_csv('test_x.csv')\n",
    "test_y = pandas.read_csv('test_y.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_final(model, test_x, test_y):\n",
    "    \n",
    "    # GET PREDICTED VALUES\n",
    "    test_predictions = model.predict(test_x)\n",
    "    \n",
    "    # GET EVALUATION NUMBERS ON TRAIN SET \n",
    "    accuracy = metrics.accuracy_score(test_y, test_predictions, normalize=True)\n",
    "    precision = metrics.precision_score(test_y, test_predictions, average='macro', zero_division='warn')\n",
    "    recall = metrics.recall_score(test_y, test_predictions, average='macro', zero_division='warn')\n",
    "    f1 = metrics.f1_score(test_y, test_predictions, average='macro', zero_division='warn')\n",
    "    \n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.9872, precision : 0.9780, recall : 0.9671, f1_score : 0.9725\n"
     ]
    }
   ],
   "source": [
    "accuracy, precision, recall, f1 = test_final(trained_model, test_x[combined_features], test_y['sentiment'])\n",
    "print (\"accuracy : %.4f, precision : %.4f, recall : %.4f, f1_score : %.4f\" %(accuracy, precision,recall,f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import test_live\n",
    "import pickle\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\salee\\anaconda3\\lib\\site-packages\\sklearn\\base.py:310: UserWarning: Trying to unpickle estimator LogisticRegression from version 0.23.1 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "best_model = pickle.load(open(\"LR\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\"هاللاعب معلم\",\n",
    "           \"هالممثل محترف\",\n",
    "           \"يا جحش\",\n",
    "           \"يا بغل\",\n",
    "           \"يا ثعلب\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pandas.read_csv('data.csv')\n",
    "# split the dataset into training and test datasets 70% training, 30% testing\n",
    "# train_x, test_x, train_y, test_y = model_selection.train_test_split(df['clean'].values.astype('U'), targets,test_size=0.3, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence = يوم وسخ \n",
      "\n",
      "Actual = hateful, \t pred = normal \n",
      "\n",
      "sentence = هو البدوي شارب بول البعير ليه بيوافق نسوانه تشوف اجسام و فخاد و حمامة رجال مفتولي العضلات و مش موافق الستات الرجال… @user \n",
      "\n",
      "Actual = normal, \t pred = hateful \n",
      "\n",
      "sentence = لسلامة الحجاج اوقفوا مواكب الامراء أثناء الحج. موكب خالد الفيصل تسبب بمقتل واصابة الالاف بتدافع منى امام البوابات المغلقه \n",
      "\n",
      "Actual = normal, \t pred = offensive \n",
      "\n",
      "sentence = مستهبل ومن لف لفيفها بصوت واحد الحق على جبران باسيل    \n",
      "\n",
      "Actual = offensive, \t pred = normal \n",
      "\n",
      "sentence = آه يا حمارويه \n",
      "\n",
      "Actual = offensive, \t pred = normal \n",
      "\n",
      "sentence = جبران باسيل مهضوم كتير قال بدو يعمر سوريا و هوي ببلدو لبنان مش قادر يعمر مجرور \n",
      "\n",
      "Actual = offensive, \t pred = normal \n",
      "\n",
      "sentence = لبيك اللهم لبيك ان الحمد والنعمت لك والملك لا شريك لك \n",
      "\n",
      "Actual = normal, \t pred = offensive \n",
      "\n",
      "sentence = إلا خصني \n",
      "\n",
      "Actual = normal, \t pred = hateful \n",
      "\n",
      "sentence = ما نايكن الا جبران باسيل عن جديد منا وجر \n",
      "\n",
      "Actual = offensive, \t pred = normal \n",
      "\n",
      "sentence = الحريم اذا شافو واحد متزوج اربع #يعجبك_السهر_اكثر_او_النوم @url \n",
      "\n",
      "Actual = normal, \t pred = hateful \n",
      "\n",
      "sentence = وحروبه مع أمير المؤمنين معاوية وام المؤمنين عائشة سببت قتل مئة ألف من خيرة الصحابة فلا أظن أن حربه للخوارج تشفع \n",
      "\n",
      "Actual = normal, \t pred = offensive \n",
      "\n",
      "sentence = كُلُّ إناءٍ ينضحُ بما فيه! \n",
      "\n",
      "Actual = normal, \t pred = hateful \n",
      "\n",
      "sentence = لعمي  \n",
      "\n",
      "Actual = normal, \t pred = hateful \n",
      "\n",
      "0.013\n"
     ]
    }
   ],
   "source": [
    "# live_test = \"هاللاعب فعلا حريف\"\n",
    "count = 0\n",
    "for i in range(1000):\n",
    "    choice = random.randrange(len(df)) \n",
    "    sen = df['tweet'][choice]\n",
    "    label = df['sentiment'][choice]\n",
    "    if len(sen) > 0:\n",
    "        live_df = test_live.prepare_live(sen)\n",
    "        pred = encoder.inverse_transform(best_model.predict(live_df))[0]\n",
    "        if pred != label:\n",
    "            count+=1\n",
    "            print(f\"sentence = {sen} \\n\")\n",
    "            print(f\"Actual = {label}, \\t pred = {pred} \\n\")\n",
    "print (float(count)/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen = \"يا بغل\"\n",
    "live_df = test_live.prepare_live(sen)\n",
    "print(encoder.inverse_transform(best_model.predict(live_df)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
