{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset And Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data, old Lexicon, new Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv')\n",
    "new_lexicon = pd.read_csv('New_Lexicon.csv')\n",
    "old_lexicon = pd.read_csv('Old_lexicon.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting Hateful Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hate_tweets = [tweet for idx, tweet in enumerate(df['clean']) \n",
    "               if df['sentiment'][idx] == 'hateful']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting the other Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_tweets = [tweet for idx, tweet in enumerate(df['clean']) if df['sentiment'][idx] != 'hateful']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification using only the lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A tweet is classified as hate-tweet if it contains a word from the lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(predictedHateTweets, hate_tweets, other_tweets):\n",
    "    FalsePositive = 0 \n",
    "    FalseNegative = 0\n",
    "    for tweet in predictedHateTweets:\n",
    "        if not tweet in hate_tweets:\n",
    "            FalsePositive+=1\n",
    "    TruePositive = len(predictedHateTweets) - FalsePositive\n",
    "    \n",
    "    for tweet in hate_tweets:\n",
    "        if not tweet in predictedHateTweets:\n",
    "            FalseNegative+=1\n",
    "    TrueNegative = len(other_tweets) - FalsePositive\n",
    "    \n",
    "    accuracy = float(TruePositive + TrueNegative)/float(TruePositive + FalsePositive + TrueNegative + FalseNegative)\n",
    "    precision = float(TruePositive)/float(TruePositive + FalsePositive)\n",
    "    recall = float(TruePositive)/float(TruePositive + FalseNegative)\n",
    "    f1_score = 2*((precision*recall)/(precision+recall))\n",
    "    \n",
    "    return accuracy, precision, recall, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification using old Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_predictedHateTweets = []\n",
    "for tweet in df['clean']:\n",
    "    for word in old_lexicon['clean']:\n",
    "        if word in str(tweet):\n",
    "            old_predictedHateTweets.append(tweet)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, precision, recall, f1_score = evaluation(old_predictedHateTweets, hate_tweets, other_tweets)\n",
    "print (\"accuracy : %.4f, precision : %.4f, recall : %.4f, f1_score : %.4f\" %(accuracy, precision,recall,f1_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification using new Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictedHateTweets = []\n",
    "for tweet in df['clean']:\n",
    "    for word in new_lexicon['clean']:\n",
    "        if word in str(tweet):\n",
    "            predictedHateTweets.append(tweet)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, precision, recall, f1_score = evaluation(predictedHateTweets, hate_tweets, other_tweets)\n",
    "print (\"accuracy : %.4f, precision : %.4f, recall : %.4f, f1_score : %.4f\" %(accuracy, precision,recall,f1_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the occurrences of lexicon's words in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Appending Corpus' words into one list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_words = []\n",
    "for tweet in df['clean']:\n",
    "    for word in str(tweet).split(\" \"):\n",
    "        dataset_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Appending Corpus' words related to hate speech into one list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hate_speech_words = []\n",
    "for tweet in hate_tweets:\n",
    "    for word in str(tweet).split(\" \"):\n",
    "        hate_speech_words.append(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FreqDist for the previous two lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_fdist = nltk.FreqDist(word for word in dataset_words)\n",
    "hate_fdist = nltk.FreqDist(word for word in hate_speech_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most 30 frequent word in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_fdist.plot(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most 30 frequent word in hateful tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hate_fdist.plot(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "old lexicon's words frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_lex_dist = {}\n",
    "for word in set(old_lexicon['clean']):\n",
    "    old_lex_dist[word] = hate_fdist[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sort new lexicon's words frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_lex_tuple = [(value, key) for key, value in old_lex_dist.items()]\n",
    "old_lex_tuple.sort(key=lambda tup: tup[0], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_lex_tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "new lexicon's words frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_lex_dist ={}\n",
    "for word in set(new_lexicon['clean']):\n",
    "    new_lex_dist[word] = hate_fdist[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sort new lexicon's words frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_lex_tuple = [(value, key) for key, value in new_lex_dist.items()]\n",
    "new_lex_tuple.sort(key=lambda tup: tup[0], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_lex_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(list(old_lex_dist.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(list(new_lex_dist.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "old Lexicon's words' categories frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_word_cat = dict()\n",
    "for cat in set(old_lexicon['category']):\n",
    "    old_word_cat[cat]= 0\n",
    "for idx, word in enumerate(old_lexicon['clean']): \n",
    "    old_word_cat[old_lexicon['category'][idx]]+= hate_fdist[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_catCount = [(cat, count) for cat,count in old_word_cat.items()]\n",
    "old_catCount.sort(key=lambda tup: tup[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_catCount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "new Lexicon's words' categories frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_word_cat = dict()\n",
    "for cat in set(new_lexicon['category']):\n",
    "    new_word_cat[cat]= 0\n",
    "for idx, word in enumerate(new_lexicon['clean']): \n",
    "    new_word_cat[new_lexicon['category'][idx]]+= hate_fdist[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_catCount = [(cat, count) for cat,count in new_word_cat.items()]\n",
    "new_catCount.sort(key=lambda tup: tup[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_catCount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extracting new words and expressions by looking at hateful_not_offinsive tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hatful_offinsive(hate_tweets):\n",
    "    hatful_offinsive_tweets = []\n",
    "    for tweet in set(hate_tweets):\n",
    "        for word in new_lexicon['clean']:\n",
    "            if word in tweet:\n",
    "                hatful_offinsive_tweets.append(tweet)\n",
    "                break\n",
    "    return hatful_offinsive_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hatful_offinsive_tweets = hatful_offinsive(hate_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hatful_not_offinsive_tweets = set(hate_tweets) - set(hatful_offinsive_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hatful_not_offinsive_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
